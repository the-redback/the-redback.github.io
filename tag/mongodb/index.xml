<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MongoDB | Maruf&#39;s</title>
    <link>https://the-maruf.com/tag/mongodb/</link>
      <atom:link href="https://the-maruf.com/tag/mongodb/index.xml" rel="self" type="application/rss+xml" />
    <description>MongoDB</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2024 Abdullah Al Maruf</copyright><lastBuildDate>Sat, 12 Jan 2019 03:07:50 +0600</lastBuildDate>
    <image>
      <url>https://the-maruf.com/media/icon_hu55f686cf01b2571760ccbbe4ddc433b7_22664_512x512_fill_lanczos_center_3.png</url>
      <title>MongoDB</title>
      <link>https://the-maruf.com/tag/mongodb/</link>
    </image>
    
    <item>
      <title>Mongodb Replica Set on Kubernetes</title>
      <link>https://the-maruf.com/blog/mongodb-replica-set-on-kubernetes/</link>
      <pubDate>Sat, 12 Jan 2019 03:07:50 +0600</pubDate>
      <guid>https://the-maruf.com/blog/mongodb-replica-set-on-kubernetes/</guid>
      <description>&lt;h3 id=&#34;replica-sets&#34;&gt;Replica-Sets&lt;/h3&gt;
&lt;p&gt;Run MongoDB Replica Set on Kubernetes using &lt;em&gt;Statefulset&lt;/em&gt; and &lt;em&gt;PersistentVolumeClaim&lt;/em&gt;. Minikube kubernetes cluster is used for this post.&lt;/p&gt;
&lt;h3 id=&#34;create-secret-for-key-file&#34;&gt;Create Secret for Key file&lt;/h3&gt;
&lt;p&gt;MongoDB will use this key to communicate internal cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ openssl rand -base64 741 &amp;gt; ./replica-sets/key.txt

$ kubectl create secret generic shared-bootstrap-data --from-file=internal-auth-mongodb-keyfile=./replica-sets/key.txt
secret &amp;quot;shared-bootstrap-data&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;deploy-mongodb-replica-sets-yaml&#34;&gt;Deploy MongoDB Replica-Sets YAML&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
  labels:
    name: mongo
spec:
  ports:
  - port: 27017
    targetPort: 27017
  clusterIP: None
  selector:
    role: mongo
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongod
spec:
  serviceName: mongodb-service
  replicas: 3
  selector:
    matchLabels:
      role: mongo
      environment: test
      replicaset: MainRepSet
  template:
    metadata:
      labels:
        role: mongo
        environment: test
        replicaset: MainRepSet
    spec:
      containers:
      - name: mongod-container
        image: mongo:3.4
        command:
        - &amp;quot;numactl&amp;quot;
        - &amp;quot;--interleave=all&amp;quot;
        - &amp;quot;mongod&amp;quot;
        - &amp;quot;--bind_ip&amp;quot;
        - &amp;quot;0.0.0.0&amp;quot;
        - &amp;quot;--replSet&amp;quot;
        - &amp;quot;MainRepSet&amp;quot;
        - &amp;quot;--auth&amp;quot;
        - &amp;quot;--clusterAuthMode&amp;quot;
        - &amp;quot;keyFile&amp;quot;
        - &amp;quot;--keyFile&amp;quot;
        - &amp;quot;/etc/secrets-volume/internal-auth-mongodb-keyfile&amp;quot;
        - &amp;quot;--setParameter&amp;quot;
        - &amp;quot;authenticationMechanisms=SCRAM-SHA-1&amp;quot;
        resources:
          requests:
            cpu: 0.2
            memory: 200Mi
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: secrets-volume
          readOnly: true
          mountPath: /etc/secrets-volume
        - name: mongodb-persistent-storage-claim
          mountPath: /data/db
      volumes:
      - name: secrets-volume
        secret:
          secretName: shared-bootstrap-data
          defaultMode: 256
  volumeClaimTemplates:
  - metadata:
      name: mongodb-persistent-storage-claim
      annotations:
        volume.beta.kubernetes.io/storage-class: &amp;quot;standard&amp;quot;
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      resources:
        requests:
          storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now Deploy the Yaml&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kc create -f ./replica-sets/mongodb-rc.yaml 
service &amp;quot;mongodb-service&amp;quot; created
statefulset &amp;quot;mongod&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;wait-for-pod-running-and-pvc&#34;&gt;Wait for Pod running and PVC&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl get all
NAME                  DESIRED   CURRENT   AGE
statefulsets/mongod   3         3         2m

NAME          READY     STATUS    RESTARTS   AGE
po/mongod-0   1/1       Running   0          2m
po/mongod-1   1/1       Running   0          2m
po/mongod-2   1/1       Running   0          2m

$ kubectl get pvc
NAME                                        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mongodb-persistent-storage-claim-mongod-0   Bound     pvc-ba24dc66-319a-11e8-8dd9-080027779e8d   1Gi        RWO            standard       1h
mongodb-persistent-storage-claim-mongod-1   Bound     pvc-bf2e51a5-319a-11e8-8dd9-080027779e8d   1Gi        RWO            standard       1h
mongodb-persistent-storage-claim-mongod-2   Bound     pvc-c7948f87-319a-11e8-8dd9-080027779e8d   1Gi        RWO            standard       1h
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setup-replicaset-configuration&#34;&gt;Setup ReplicaSet Configuration&lt;/h3&gt;
&lt;p&gt;Finally, we need to connect to one of the “mongod” container processes to configure the replica set.&lt;/p&gt;
&lt;p&gt;Run the following command to connect to the first container. In the shell initiate the replica set (we can rely on the hostnames always being the same, due to having employed a StatefulSet):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl exec -it mongod-0 -c mongod-container bash
$ mongo
&amp;gt; rs.initiate({_id: &amp;quot;MainRepSet&amp;quot;, version: 1, members: [
       { _id: 0, host : &amp;quot;mongod-0.mongodb-service.default.svc.cluster.local:27017&amp;quot; },
       { _id: 1, host : &amp;quot;mongod-1.mongodb-service.default.svc.cluster.local:27017&amp;quot; },
       { _id: 2, host : &amp;quot;mongod-2.mongodb-service.default.svc.cluster.local:27017&amp;quot; }
 ]});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep checking the status of the replica set, with the following command, until the replica set is fully initialised and a primary and two secondaries are present:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;gt; rs.status();

# output similar to:
{
	&amp;quot;set&amp;quot; : &amp;quot;MainRepSet&amp;quot;,
	&amp;quot;date&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:31.577Z&amp;quot;),
	&amp;quot;myState&amp;quot; : 2,
	&amp;quot;term&amp;quot; : NumberLong(1),
	&amp;quot;syncingTo&amp;quot; : &amp;quot;mongod-2.mongodb-service.default.svc.cluster.local:27017&amp;quot;,
	&amp;quot;heartbeatIntervalMillis&amp;quot; : NumberLong(2000),
	&amp;quot;optimes&amp;quot; : {
		&amp;quot;lastCommittedOpTime&amp;quot; : {
			&amp;quot;ts&amp;quot; : Timestamp(1522152676, 1),
			&amp;quot;t&amp;quot; : NumberLong(1)
		},
		&amp;quot;appliedOpTime&amp;quot; : {
			&amp;quot;ts&amp;quot; : Timestamp(1522152686, 1),
			&amp;quot;t&amp;quot; : NumberLong(1)
		},
		&amp;quot;durableOpTime&amp;quot; : {
			&amp;quot;ts&amp;quot; : Timestamp(1522152686, 1),
			&amp;quot;t&amp;quot; : NumberLong(1)
		}
	},
	&amp;quot;members&amp;quot; : [
		{
			&amp;quot;_id&amp;quot; : 0,
			&amp;quot;name&amp;quot; : &amp;quot;mongod-0.mongodb-service.default.svc.cluster.local:27017&amp;quot;,
			&amp;quot;health&amp;quot; : 1,
			&amp;quot;state&amp;quot; : 1,
			&amp;quot;stateStr&amp;quot; : &amp;quot;PRIMARY&amp;quot;,
			&amp;quot;uptime&amp;quot; : 399,
			&amp;quot;optime&amp;quot; : {
				&amp;quot;ts&amp;quot; : Timestamp(1522152686, 1),
				&amp;quot;t&amp;quot; : NumberLong(1)
			},
			&amp;quot;optimeDurable&amp;quot; : {
				&amp;quot;ts&amp;quot; : Timestamp(1522152686, 1),
				&amp;quot;t&amp;quot; : NumberLong(1)
			},
			&amp;quot;optimeDate&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:26Z&amp;quot;),
			&amp;quot;optimeDurableDate&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:26Z&amp;quot;),
			&amp;quot;lastHeartbeat&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:30.360Z&amp;quot;),
			&amp;quot;lastHeartbeatRecv&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:30.697Z&amp;quot;),
			&amp;quot;pingMs&amp;quot; : NumberLong(0),
			&amp;quot;electionTime&amp;quot; : Timestamp(1522152306, 1),
			&amp;quot;electionDate&amp;quot; : ISODate(&amp;quot;2018-03-27T12:05:06Z&amp;quot;),
			&amp;quot;configVersion&amp;quot; : 1
		},
		{
			&amp;quot;_id&amp;quot; : 1,
			&amp;quot;name&amp;quot; : &amp;quot;mongod-1.mongodb-service.default.svc.cluster.local:27017&amp;quot;,
			&amp;quot;health&amp;quot; : 1,
			&amp;quot;state&amp;quot; : 2,
			&amp;quot;stateStr&amp;quot; : &amp;quot;SECONDARY&amp;quot;,
			&amp;quot;uptime&amp;quot; : 505,
			&amp;quot;optime&amp;quot; : {
				&amp;quot;ts&amp;quot; : Timestamp(1522152686, 1),
				&amp;quot;t&amp;quot; : NumberLong(1)
			},
			&amp;quot;optimeDate&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:26Z&amp;quot;),
			&amp;quot;syncingTo&amp;quot; : &amp;quot;mongod-2.mongodb-service.default.svc.cluster.local:27017&amp;quot;,
			&amp;quot;configVersion&amp;quot; : 1,
			&amp;quot;self&amp;quot; : true
		},
		{
			&amp;quot;_id&amp;quot; : 2,
			&amp;quot;name&amp;quot; : &amp;quot;mongod-2.mongodb-service.default.svc.cluster.local:27017&amp;quot;,
			&amp;quot;health&amp;quot; : 1,
			&amp;quot;state&amp;quot; : 2,
			&amp;quot;stateStr&amp;quot; : &amp;quot;SECONDARY&amp;quot;,
			&amp;quot;uptime&amp;quot; : 399,
			&amp;quot;optime&amp;quot; : {
				&amp;quot;ts&amp;quot; : Timestamp(1522152686, 1),
				&amp;quot;t&amp;quot; : NumberLong(1)
			},
			&amp;quot;optimeDurable&amp;quot; : {
				&amp;quot;ts&amp;quot; : Timestamp(1522152686, 1),
				&amp;quot;t&amp;quot; : NumberLong(1)
			},
			&amp;quot;optimeDate&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:26Z&amp;quot;),
			&amp;quot;optimeDurableDate&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:26Z&amp;quot;),
			&amp;quot;lastHeartbeat&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:30.360Z&amp;quot;),
			&amp;quot;lastHeartbeatRecv&amp;quot; : ISODate(&amp;quot;2018-03-27T12:11:29.915Z&amp;quot;),
			&amp;quot;pingMs&amp;quot; : NumberLong(0),
			&amp;quot;syncingTo&amp;quot; : &amp;quot;mongod-0.mongodb-service.default.svc.cluster.local:27017&amp;quot;,
			&amp;quot;configVersion&amp;quot; : 1
		}
	],
	&amp;quot;ok&amp;quot; : 1
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mongodb-0&lt;/code&gt; has become &lt;code&gt;Primary&lt;/code&gt; and Others two &lt;code&gt;Secondary&lt;/code&gt; Nodes.&lt;/p&gt;
&lt;p&gt;Then run the following command to configure an “admin” user (performing this action results in the “localhost exception” being automatically and permanently disabled):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; db.getSiblingDB(&amp;quot;admin&amp;quot;).createUser({
      user : &amp;quot;main_admin&amp;quot;,
      pwd  : &amp;quot;abc123&amp;quot;,
      roles: [ { role: &amp;quot;root&amp;quot;, db: &amp;quot;admin&amp;quot; } ]
 });
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;insert-data&#34;&gt;Insert Data&lt;/h3&gt;
&lt;p&gt;Insert Data into &lt;code&gt;mongod-0&lt;/code&gt; pod.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;&amp;gt; db.getSiblingDB(&#39;admin&#39;).auth(&amp;quot;main_admin&amp;quot;, &amp;quot;abc123&amp;quot;);
&amp;gt; use test;
&amp;gt; db.testcoll.insert({a:1});
&amp;gt; db.testcoll.insert({b:2});
&amp;gt; db.testcoll.find();
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;verify-cluster-data&#34;&gt;Verify Cluster Data&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;exec&lt;/code&gt; into Secondary Pod (here, mongo-1)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl exec -it mongod-1 -c mongod-container bash
$ mongo
&amp;gt; db.getSiblingDB(&#39;admin&#39;).auth(&amp;quot;main_admin&amp;quot;, &amp;quot;abc123&amp;quot;);
&amp;gt; db.getMongo().setSlaveOk()
&amp;gt; use test;
&amp;gt; db.testcoll.find();
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;verify-pvc&#34;&gt;Verify PVC&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl delete -f ./replica-sets/mongodb-rc.yaml
$ kubectl get all
$ kubectl get persistentvolumes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recreate MongoDB&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl apply -f ./replica-sets/mongodb-rc.yaml
$ kubectl get all
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Verify Data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ kubectl exec -it mongod-0 -c mongod-container bash
$ mongo
&amp;gt; db.getSiblingDB(&#39;admin&#39;).auth(&amp;quot;main_admin&amp;quot;, &amp;quot;abc123&amp;quot;);
&amp;gt; use test;
&amp;gt; db.testcoll.find();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As PVC was not deleted, We will still have existing Data.&lt;/p&gt;
&lt;h3 id=&#34;verify-clusterization&#34;&gt;Verify Clusterization&lt;/h3&gt;
&lt;p&gt;Delete &lt;code&gt;mongod-0&lt;/code&gt; Pod and keep cheking &lt;code&gt;rs.status()&lt;/code&gt;, eventually another node of the remaining two will become &lt;code&gt;Primary&lt;/code&gt; Node.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
