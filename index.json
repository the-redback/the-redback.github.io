[{"authors":[],"categories":[],"content":" Replica-Sets Create Secret for Key file MongoDB will use this key to communicate internal cluster.\n$ openssl rand -base64 741 \u0026gt; ./replica-sets/key.txt $ kubectl create secret generic shared-bootstrap-data --from-file=internal-auth-mongodb-keyfile=./replica-sets/key.txt secret \u0026quot;shared-bootstrap-data\u0026quot; created  Deploy MongoDB Replica-Sets YAML apiVersion: v1 kind: Service metadata: name: mongodb-service labels: name: mongo spec: ports: - port: 27017 targetPort: 27017 clusterIP: None selector: role: mongo --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mongod spec: serviceName: mongodb-service replicas: 3 selector: matchLabels: role: mongo environment: test replicaset: MainRepSet template: metadata: labels: role: mongo environment: test replicaset: MainRepSet spec: containers: - name: mongod-container image: mongo:3.4 command: - \u0026quot;numactl\u0026quot; - \u0026quot;--interleave=all\u0026quot; - \u0026quot;mongod\u0026quot; - \u0026quot;--bind_ip\u0026quot; - \u0026quot;0.0.0.0\u0026quot; - \u0026quot;--replSet\u0026quot; - \u0026quot;MainRepSet\u0026quot; - \u0026quot;--auth\u0026quot; - \u0026quot;--clusterAuthMode\u0026quot; - \u0026quot;keyFile\u0026quot; - \u0026quot;--keyFile\u0026quot; - \u0026quot;/etc/secrets-volume/internal-auth-mongodb-keyfile\u0026quot; - \u0026quot;--setParameter\u0026quot; - \u0026quot;authenticationMechanisms=SCRAM-SHA-1\u0026quot; resources: requests: cpu: 0.2 memory: 200Mi ports: - containerPort: 27017 volumeMounts: - name: secrets-volume readOnly: true mountPath: /etc/secrets-volume - name: mongodb-persistent-storage-claim mountPath: /data/db volumes: - name: secrets-volume secret: secretName: shared-bootstrap-data defaultMode: 256 volumeClaimTemplates: - metadata: name: mongodb-persistent-storage-claim annotations: volume.beta.kubernetes.io/storage-class: \u0026quot;standard\u0026quot; spec: accessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ] resources: requests: storage: 1Gi  Now Deploy the Yaml\n$ kc create -f ./replica-sets/mongodb-rc.yaml service \u0026quot;mongodb-service\u0026quot; created statefulset \u0026quot;mongod\u0026quot; created  Wait for Pod running and PVC $ kubectl get all NAME DESIRED CURRENT AGE statefulsets/mongod 3 3 2m NAME READY STATUS RESTARTS AGE po/mongod-0 1/1 Running 0 2m po/mongod-1 1/1 Running 0 2m po/mongod-2 1/1 Running 0 2m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-persistent-storage-claim-mongod-0 Bound pvc-ba24dc66-319a-11e8-8dd9-080027779e8d 1Gi RWO standard 1h mongodb-persistent-storage-claim-mongod-1 Bound pvc-bf2e51a5-319a-11e8-8dd9-080027779e8d 1Gi RWO standard 1h mongodb-persistent-storage-claim-mongod-2 Bound pvc-c7948f87-319a-11e8-8dd9-080027779e8d 1Gi RWO standard 1h  Setup ReplicaSet Configuration Finally, we need to connect to one of the “mongod” container processes to configure the replica set.\nRun the following command to connect to the first container. In the shell initiate the replica set (we can rely on the hostnames always being the same, due to having employed a StatefulSet):\n$ kubectl exec -it mongod-0 -c mongod-container bash $ mongo \u0026gt; rs.initiate({_id: \u0026quot;MainRepSet\u0026quot;, version: 1, members: [ { _id: 0, host : \u0026quot;mongod-0.mongodb-service.default.svc.cluster.local:27017\u0026quot; }, { _id: 1, host : \u0026quot;mongod-1.mongodb-service.default.svc.cluster.local:27017\u0026quot; }, { _id: 2, host : \u0026quot;mongod-2.mongodb-service.default.svc.cluster.local:27017\u0026quot; } ]});  Keep checking the status of the replica set, with the following command, until the replica set is fully initialised and a primary and two secondaries are present:\n\u0026gt; rs.status(); # output similar to: { \u0026quot;set\u0026quot; : \u0026quot;MainRepSet\u0026quot;, \u0026quot;date\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:31.577Z\u0026quot;), \u0026quot;myState\u0026quot; : 2, \u0026quot;term\u0026quot; : NumberLong(1), \u0026quot;syncingTo\u0026quot; : \u0026quot;mongod-2.mongodb-service.default.svc.cluster.local:27017\u0026quot;, \u0026quot;heartbeatIntervalMillis\u0026quot; : NumberLong(2000), \u0026quot;optimes\u0026quot; : { \u0026quot;lastCommittedOpTime\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152676, 1), \u0026quot;t\u0026quot; : NumberLong(1) }, \u0026quot;appliedOpTime\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152686, 1), \u0026quot;t\u0026quot; : NumberLong(1) }, \u0026quot;durableOpTime\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152686, 1), \u0026quot;t\u0026quot; : NumberLong(1) } }, \u0026quot;members\u0026quot; : [ { \u0026quot;_id\u0026quot; : 0, \u0026quot;name\u0026quot; : \u0026quot;mongod-0.mongodb-service.default.svc.cluster.local:27017\u0026quot;, \u0026quot;health\u0026quot; : 1, \u0026quot;state\u0026quot; : 1, \u0026quot;stateStr\u0026quot; : \u0026quot;PRIMARY\u0026quot;, \u0026quot;uptime\u0026quot; : 399, \u0026quot;optime\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152686, 1), \u0026quot;t\u0026quot; : NumberLong(1) }, \u0026quot;optimeDurable\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152686, 1), \u0026quot;t\u0026quot; : NumberLong(1) }, \u0026quot;optimeDate\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:26Z\u0026quot;), \u0026quot;optimeDurableDate\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:26Z\u0026quot;), \u0026quot;lastHeartbeat\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:30.360Z\u0026quot;), \u0026quot;lastHeartbeatRecv\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:30.697Z\u0026quot;), \u0026quot;pingMs\u0026quot; : NumberLong(0), \u0026quot;electionTime\u0026quot; : Timestamp(1522152306, 1), \u0026quot;electionDate\u0026quot; : ISODate(\u0026quot;2018-03-27T12:05:06Z\u0026quot;), \u0026quot;configVersion\u0026quot; : 1 }, { \u0026quot;_id\u0026quot; : 1, \u0026quot;name\u0026quot; : \u0026quot;mongod-1.mongodb-service.default.svc.cluster.local:27017\u0026quot;, \u0026quot;health\u0026quot; : 1, \u0026quot;state\u0026quot; : 2, \u0026quot;stateStr\u0026quot; : \u0026quot;SECONDARY\u0026quot;, \u0026quot;uptime\u0026quot; : 505, \u0026quot;optime\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152686, 1), \u0026quot;t\u0026quot; : NumberLong(1) }, \u0026quot;optimeDate\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:26Z\u0026quot;), \u0026quot;syncingTo\u0026quot; : \u0026quot;mongod-2.mongodb-service.default.svc.cluster.local:27017\u0026quot;, \u0026quot;configVersion\u0026quot; : 1, \u0026quot;self\u0026quot; : true }, { \u0026quot;_id\u0026quot; : 2, \u0026quot;name\u0026quot; : \u0026quot;mongod-2.mongodb-service.default.svc.cluster.local:27017\u0026quot;, \u0026quot;health\u0026quot; : 1, \u0026quot;state\u0026quot; : 2, \u0026quot;stateStr\u0026quot; : \u0026quot;SECONDARY\u0026quot;, \u0026quot;uptime\u0026quot; : 399, \u0026quot;optime\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152686, 1), \u0026quot;t\u0026quot; : NumberLong(1) }, \u0026quot;optimeDurable\u0026quot; : { \u0026quot;ts\u0026quot; : Timestamp(1522152686, 1), \u0026quot;t\u0026quot; : NumberLong(1) }, \u0026quot;optimeDate\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:26Z\u0026quot;), \u0026quot;optimeDurableDate\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:26Z\u0026quot;), \u0026quot;lastHeartbeat\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:30.360Z\u0026quot;), \u0026quot;lastHeartbeatRecv\u0026quot; : ISODate(\u0026quot;2018-03-27T12:11:29.915Z\u0026quot;), \u0026quot;pingMs\u0026quot; : NumberLong(0), \u0026quot;syncingTo\u0026quot; : \u0026quot;mongod-0.mongodb-service.default.svc.cluster.local:27017\u0026quot;, \u0026quot;configVersion\u0026quot; : 1 } ], \u0026quot;ok\u0026quot; : 1 }  mongodb-0 has become Primary and Others two Secondary Nodes.\nThen run the following command to configure an “admin” user (performing this action results in the “localhost exception” being automatically and permanently disabled):\n\u0026gt; db.getSiblingDB(\u0026quot;admin\u0026quot;).createUser({ user : \u0026quot;main_admin\u0026quot;, pwd : \u0026quot;abc123\u0026quot;, roles: [ { role: \u0026quot;root\u0026quot;, db: \u0026quot;admin\u0026quot; } ] });  Insert Data Insert Data into mongod-0 pod.\n\u0026gt; db.getSiblingDB('admin').auth(\u0026quot;main_admin\u0026quot;, \u0026quot;abc123\u0026quot;); \u0026gt; use test; \u0026gt; db.testcoll.insert({a:1}); \u0026gt; db.testcoll.insert({b:2}); \u0026gt; db.testcoll.find();  Verify Cluster Data exec into Secondary Pod (here, mongo-1)\n$ kubectl exec -it mongod-1 -c mongod-container bash $ mongo \u0026gt; db.getSiblingDB('admin').auth(\u0026quot;main_admin\u0026quot;, \u0026quot;abc123\u0026quot;); \u0026gt; db.getMongo().setSlaveOk() \u0026gt; use test; \u0026gt; db.testcoll.find();  Verify PVC $ kubectl delete -f ./replica-sets/mongodb-rc.yaml $ kubectl get all $ kubectl get persistentvolumes  Recreate MongoDB\n$ kubectl apply -f ./replica-sets/mongodb-rc.yaml $ kubectl get all  Verify Data:\n$ kubectl exec -it mongod-0 -c mongod-container bash $ mongo \u0026gt; db.getSiblingDB('admin').auth(\u0026quot;main_admin\u0026quot;, \u0026quot;abc123\u0026quot;); \u0026gt; use test; \u0026gt; db.testcoll.find();  As PVC was not deleted, We will still have existing Data.\nVerify Clusterization Delete mongod-0 Pod and keep cheking rs.status(), eventually another node of the remaining two will become Primary Node.\n","date":1547240870,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547240870,"objectID":"0acc8f44a4a00babc8f19ee0bb12bfa4","permalink":"https://maruftuhin.com/post/mongodb-on-kubernetes/","publishdate":"2019-01-12T03:07:50+06:00","relpermalink":"/post/mongodb-on-kubernetes/","section":"post","summary":"Replica-Sets Create Secret for Key file MongoDB will use this key to communicate internal cluster.\n$ openssl rand -base64 741 \u0026gt; ./replica-sets/key.txt $ kubectl create secret generic shared-bootstrap-data --from-file=internal-auth-mongodb-keyfile=./replica-sets/key.txt secret \u0026quot;shared-bootstrap-data\u0026quot; created  Deploy MongoDB Replica-Sets YAML apiVersion: v1 kind: Service metadata: name: mongodb-service labels: name: mongo spec: ports: - port: 27017 targetPort: 27017 clusterIP: None selector: role: mongo --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mongod spec: serviceName: mongodb-service replicas: 3 selector: matchLabels: role: mongo environment: test replicaset: MainRepSet template: metadata: labels: role: mongo environment: test replicaset: MainRepSet spec: containers: - name: mongod-container image: mongo:3.","tags":[],"title":"Mongodb on Kubernetes","type":"post"}]