[{"authors":["admin"],"categories":null,"content":" Hey, I\u0026#39;m Maruf! I like to tear down build stuff!! üòâ I am a passionate software engineer with 4 years of professional working experience in both developing Kubernetes tools in GO and managing microservice based infrastructure in production. Acted as AppsCode‚Äôs KubeDB project head for more than a year. Love to participate in programming contests. An enthusiastic team player.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://maruftuhin.com/author/abdullah-al-maruf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/abdullah-al-maruf/","section":"authors","summary":"Hey, I'm Maruf! I like to tear down build stuff!! üòâ I am a passionate software engineer with 4 years of professional working experience in both developing Kubernetes tools in GO and managing microservice based infrastructure in production.","tags":null,"title":"Abdullah Al Maruf","type":"authors"},{"authors":["Abdullah Al Maruf","Alexander Bakhtin","Tomas Cerny","Davide Taibi"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657224469,"objectID":"156e5d50d99b8691e547a76d54d15fb3","permalink":"https://maruftuhin.com/publication/maruf-using-2022/","publishdate":"2022-07-07T20:07:49.47608Z","relpermalink":"/publication/maruf-using-2022/","section":"publication","summary":"Microservices bring various benefits to software systems. They also bring decentralization and lose coupling across self-contained system parts. Since these systems likely evolve in a decentralized manner, they need to be monitored to identify when possibly poorly designed extensions deteriorate the overall system quality. For monolith systems, such tasks have been commonly addressed through static analysis. However, given the decentralization and possible language diversity across microservices, static analysis tools are lacking. On the other hand, there are available tools commonly used by practitioners that offer centralized logging, tracing, and metric collection for microservices. In this paper, we assess the opportunity to combine current dynamic analysis tools with anomaly detection in the form of quality metrics and anti-patterns. We develop a tool prototype that we use to assess a large microservice system benchmark demonstrating the feasibility and potential of such an approach.","tags":["Computer Science - Software Engineering"],"title":"Using Microservice Telemetry Data for System Dynamic Analysis","type":"publication"},{"authors":["Md Rofiqul Islam","Abdullah Al Maruf","Tomas Cerny"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657224469,"objectID":"1541abdc1800e5a6394ed735c98c7e25","permalink":"https://maruftuhin.com/publication/electronics-11121880/","publishdate":"2022-07-07T20:07:49.021734Z","relpermalink":"/publication/electronics-11121880/","section":"publication","summary":"One of the most significant impediments to the long-term maintainability of software applications is code smells. Keeping up with the best coding practices can be difficult for software developers, which might lead to performance throttling or code maintenance concerns. As a result, it is imperative that large applications be regularly monitored for performance issues and code smells, so that these issues can be corrected promptly. Resolving code smells in software systems can be done in a variety of ways, but doing so all at once would be prohibitively expensive and can be out of budget. Prioritizing these solutions are therefore critical. The majority of current research prioritizes code smells according to the type of smell they cause. This method, however, is not sufficient because of a lack of knowledge regarding the frequency of code usage and code changeability behavior. Even the most complex programs have some components that are more important than others. Maintaining the functionality of certain parts is essential since they are often used. Identifying and correcting code smells in places that are frequently utilized and subject to rapid change should take precedence over other code smells. A novel strategy is proposed for finding frequently used and change-prone areas in a codebase by combining business logic, heat map information, and commit history analysis in this study. It examines the codebase, commits, and log files of Java applications to identify business processes, heat map graphs, and severity levels of various types of code smells and their commit history. This is done in order to present a comprehensive, efficient, and resource-friendly technique for identifying and prioritizing performance throttling with also handling code maintenance concerns.","tags":[],"title":"Code Smell Prioritization with Business Process Mining and Static Code Analysis: A Case Study","type":"publication"},{"authors":["Alexander Bakhtin","Abdullah Al Maruf","Tomas Cerny","Davide Taibi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657224468,"objectID":"eeaa3bef3f10e4cc68252d099f35ba68","permalink":"https://maruftuhin.com/publication/bakhtin-survey-2022/","publishdate":"2022-07-07T20:07:48.709842Z","relpermalink":"/publication/bakhtin-survey-2022/","section":"publication","summary":"It is well recognized that design patterns improve system development and maintenance in many aspects. While we commonly recognize these patterns in monolithic systems, many patterns emerged for cloud computing, specifically microservices. Unfortunately, while various patterns have been proposed, available quality assessment tools often do not recognize many. This article performs a grey literature review to find and catalog available tools to detect microservice API patterns (MAP). It reasons about mechanisms that can be used to detect these patterns. Furthermore, the results indicate gaps and opportunities for improvements for quality assessment tools. Finally, the reader is provided with a route map to detection techniques that can be used to mine MAPs.","tags":["Software Engineering (cs.SE)","FOS: Computer and information sciences","FOS: Computer and information sciences"],"title":"Survey on Tools and Techniques Detecting Microservice API Patterns","type":"publication"},{"authors":["Dipta Das","Abdullah Al Maruf","Rofiqul Islam","Noah Lambaria","Samuel Kim","Amr S. Abdelfattah","Tomas Cerny","Karel Frajtak","Miroslav Bures","Pavel Tisnovsky"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657224469,"objectID":"2f94cb4086885c9d2047a3863c09955d","permalink":"https://maruftuhin.com/publication/10-1145-3512753-3512755/","publishdate":"2022-07-07T20:07:48.867129Z","relpermalink":"/publication/10-1145-3512753-3512755/","section":"publication","summary":"Poor design choices, bad coding practices, or the need to produce software quickly can stand behind technical debt. Unfortunately, manually identifying and managing technical debt gets more difficult as the software matures. Recent research offers various techniques to automate the process of detecting and managing technical debt to address these challenges. This manuscript presents a mapping study of the many aspects of technical debt that have been discovered in this field of study. This includes looking at the various forms of technical debt, as well as detection methods, the financial implications, and mitigation strategies. The findings and outcomes of this study are applicable to a wide range of software development life-cycle decisions.","tags":["code smells","architectural debt","technical debt","code debt","design debt","architectural degradation"],"title":"Technical Debt Resulting from Architectural Degradation and Code Smells: A Systematic Mapping Study","type":"publication"},{"authors":["Vincent Bushong","Amr S. Abdelfattah","Abdullah A. Maruf","Dipta Das","Austin Lehman","Eric Jaroszewski","Michael Coffey","Tomas Cerny","Karel Frajtak","Pavel Tisnovsky","Miroslav Bures"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657224468,"objectID":"ace31c70e664366a6e4dbcb384c113b3","permalink":"https://maruftuhin.com/publication/app-11177856/","publishdate":"2022-07-07T20:07:48.560671Z","relpermalink":"/publication/app-11177856/","section":"publication","summary":"Microservice architecture has become the leading design for cloud-native systems. The highly decentralized approach to software development consists of relatively independent services, which provides benefits such as faster deployment cycles, better scalability, and good separation of concerns among services. With this new architecture, one can naturally expect a broad range of advancements and simplifications over legacy systems. However, microservice system design remains challenging, as it is still difficult for engineers to understand the system module boundaries. Thus, understanding and explaining the microservice systems might not be as easy as initially thought. This study aims to classify recently published approaches and techniques to analyze microservice systems. It also looks at the evolutionary perspective of such systems and their analysis. Furthermore, the identified approaches target various challenges and goals, which this study analyzed. Thus, it provides the reader with a roadmap to the discipline, tools, techniques, and open challenges for future work. It provides a guide towards choices when aiming for analyzing cloud-native systems. The results indicate five analytical approaches commonly used in the literature, possibly in combination, towards problems classified into seven categories.","tags":[],"title":"On Microservice Analysis and Architecture Evolution: A Systematic Mapping Study","type":"publication"},{"authors":["Vincent Bushong","Dipta Das","Abdullah Al Maruf","Tomas Cerny"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657224469,"objectID":"2f29ebfd4298f14dfeda6c48684e0138","permalink":"https://maruftuhin.com/publication/9678749/","publishdate":"2022-07-07T20:07:49.326056Z","relpermalink":"/publication/9678749/","section":"publication","summary":"","tags":[],"title":"Using Static Analysis to Address Microservice Architecture Reconstruction","type":"publication"},{"authors":["Abdullah Al Maruf","Noah Lambaria","Amr S. Abdelfattah","Tomas Cerny"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657224469,"objectID":"c50552cd84c517b85364bc23127cc2f8","permalink":"https://maruftuhin.com/publication/9678532/","publishdate":"2022-07-07T20:07:49.172864Z","relpermalink":"/publication/9678532/","section":"publication","summary":"","tags":[],"title":"Using Version Control and Issue Tickets to detect Code Debt and Economical Cost","type":"publication"},{"authors":[],"categories":["Kubernetes","kubectl","Minikube"],"content":"Kubectl Cheat Sheet \u0026gt; minikube version \u0026gt; minikube start \u0026gt; kubectl version \u0026gt; kubectl get nodes Run app in kubernetes Run app in kubernetes by kubectl\n\u0026gt; kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080 also run GRPS_Sample //problematic\n\u0026gt; kubectl run kubernetes-grpc-sample --image=maruftuhin/server_grpc_sample --port=8088 See deployments\n\u0026gt; kubectl get deployments run proxy and echo\n\u0026gt; kubectl proxy setting up POD_NAME variable\n\u0026gt; export POD_NAME=$(kubectl get pods -o go-template --template \u0026#39;{{range .items}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39;) \u0026gt; echo Name of the Pod: $POD_NAME \u0026gt; curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/$POD_NAME Pods \u0026gt; kubectl get pods \u0026gt; kubectl describe pods \u0026gt; kubectl logs $POD_NAME Execute commands on container settings up env variables\n\u0026gt; kubectl exec $POD_NAME env run a container\n\u0026gt; kubectl exec -ti $POD_NAME bash \u0026gt; exit Services on kubernetes \u0026gt; kubectl get services create and expose new service\n\u0026gt; kubectl expose deployment/kubernetes-bootcamp --type=\u0026#34;NodePort\u0026#34; --port 8080 setting up Node Port variable\n\u0026gt; export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template=\u0026#39;{{(index .spec.ports 0).nodePort}}\u0026#39;) \u0026gt; echo NODE_PORT=$NODE_PORT accessing POD from external\n\u0026gt; curl host01:$NODE_PORT accessing pod from internal\n\u0026gt; kubectl exec -ti $POD_NAME curl localhost:8080 get description of only one deployment\n\u0026gt; kubectl get pods -l \u0026lt;label\u0026gt; i.e. \u0026gt; kubectl get pods -l run=kubernetes-bootcamp \u0026gt; kubectl get services -l run=kubernetes-bootcamp setting up lable. 1st get POD NAME. then \u0026gt; export POD_NAME=$(kubectl get pods -o go-template --template \u0026#39;{{range .items}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39;) \u0026gt; echo Name of the Pod: $POD_NAME label it \u0026gt; kubectl label pod $POD_NAME app=v1 List of pods using label ‚Äúapp=v1‚Äù\n\u0026gt; kubectl get pods -l app=v1 delete services with label run=kubernetes-bootcamp\n\u0026gt; kubectl delete service -l run=kubernetes-bootcamp Scaling \u0026gt; kubectl get deployments scale to 4 replicas. [kubernetes‚Äìbootmcamp] is name of deloyments\n\u0026gt; kubectl scale deployments/kubernetes-bootcamp --replicas=4 current pods\n\u0026gt; kubectl get pods -o wide Change log\n\u0026gt; kubectl describe deployments/kubernetes-bootcamp find exposed ip and service\n\u0026gt; kubectl describe services/kubernetes-bootcamp set environment variable NODE PORT\n\u0026gt; export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template=\u0026#39;{{(index .spec.ports 0).nodePort}}\u0026#39;) \u0026gt; echo NODE_PORT=$NODE_PORT Run this command to see which pod is serving\n\u0026gt; curl host01:$NODE_PORT Scale down service to 2 replicas from 4 replicas\n\u0026gt; kubectl scale deployments/kubernetes-bootcamp --replicas=2 Description again\n\u0026gt; kubectl get deployments \u0026gt; kubectl get pods -o wide Updating Application \u0026gt; kubectl get deployments \u0026gt; kubectl get pods current image version of app\n\u0026gt; kubectl describe pods update the image to version 2\n\u0026gt; kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 Changes in Pods\n\u0026gt; kubectl get pods Verify update service description\n\u0026gt; kubectl describe services/kubernetes-bootcamp setting up NODE PORT\n\u0026gt; export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template=\u0026#39;{{(index .spec.ports 0).nodePort}}\u0026#39;) \u0026gt; echo NODE_PORT=$NODE_PORT rollout status command to see update\n\u0026gt; kubectl rollout status deployments/kubernetes-bootcamp Rollback update First deploy update tag v10 for example\n\u0026gt; kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v10 status\n\u0026gt; kubectl get deployments \u0026gt; kubectl get pods \u0026gt; kubectl describe pods # we can see, not all are v10 applied rollback to previous working state\n\u0026gt; kubectl rollout undo deployments/kubernetes-bootcamp Run own program docker-gRPC \u0026gt; docker images \u0026gt; kubectl run grpc-sample3 --image=maruftuhin/server_grpc_sample:tag --port=8088 \u0026gt; kubectl get pods access from internal\n\u0026gt; kubectl proxy \u0026gt; curl --data \u0026#34;{\\\u0026#34;name\\\u0026#34;: \\\u0026#34;pc1\\\u0026#34;, \\\u0026#34;a\\\u0026#34;:2,\\\u0026#34;b\\\u0026#34;:3}\u0026#34; http://localhost:8001/api/v1/proxy/namespaces/default/pods/\u0026lt;pods id \u0026gt;/echo access from external [Expose ]\n\u0026gt; kubectl expose deployment/grpc-sample3 --type=\u0026#34;NodePort\u0026#34; --port 8088 \u0026gt; kubectl get services/grpc-sample3 setting $Node_Port variable. but the ports can also be used from ‚Äúkubectl get services/grpc-sample3‚Äù results\n\u0026gt; export NODE_PORT=$(kubectl get services/grpc-sample3 -o go-template=\u0026#39;{{(index .spec.ports 0).nodePort}}\u0026#39;) \u0026gt; kubectl get services/grpc-sample3 \u0026gt; kubectl get nodes \u0026gt; kubectl get svc grpc-sample3 -o yaml \u0026gt; minikube service grpc-sample3 --url \u0026gt; curl --data \u0026#34;{\\\u0026#34;name\\\u0026#34;: \\\u0026#34;pc1\\\u0026#34;, \\\u0026#34;a\\\u0026#34;:2,\\\u0026#34;b\\\u0026#34;:3}\u0026#34; http://192.168.99.100:32258/echo ","date":1551550510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551550510,"objectID":"e4bd07e84a85576bda394202fe5b95da","permalink":"https://maruftuhin.com/blog/my-kubectl-cheat-sheet/","publishdate":"2019-03-03T00:15:10+06:00","relpermalink":"/blog/my-kubectl-cheat-sheet/","section":"post","summary":"Essential `kubectl` commands","tags":["Kubernetes","kubectl","Container","Minikube"],"title":"My Kubectl Cheat Sheet","type":"post"},{"authors":[],"categories":["Docker","Kubernetes","Bash"],"content":"Docker Cheat Sheet Build and run a docker image Build docker images from docker file, goto dockerfile directory and command:\ndocker build -t friendlynginx . Run docker image from locally built image\ndocker run -d -p 50053:8088 friendlynginx # -d for running it in background //50053:8088 is the exposed port Connect with docker hub and push to directory, a directory maruftuhin/nginx is created i docker hub at first, then login\ndocker login docker tag friendlynginx maruftuhin/nginx:tag docker push maruftuhin/nginx Pull image from docker hub:\ndocker pull maruftuhin/nginx:tag Run image using tag , [if locally unavailable, it pulls from online]\ndocker run -p -d 4000:8088 maruftuhin/nginx:tag List of images and containers List of images\ndocker images List all exited containers\ndocker ps -a -q -f status=exited List of containers\ndocker ps -a # -a for all Stop/remove a container or image Stop a container\ndocker stop \u0026lt;container_id\u0026gt; Start a exited container\ndocker start \u0026lt;container id\u0026gt; Remove an image\ndocker rmi \u0026lt;image_id\u0026gt; Remove a container\ndocker rm \u0026lt;container id\u0026gt; docker rm \u0026lt;container_name\u0026gt; Remove more than one image/container\ndocker rm \u0026lt;container id\u0026gt; \u0026lt;container_id\u0026gt; Making change in a container/image Creates a container for a image and enters it directory\ndocker run -it \u0026lt;image_id\u0026gt; /bin/bash Insert in a existing container [the container must be UP]\ndocker exec -it \u0026lt;container_id\u0026gt; bash # example: docker exec -it ecf1310ac3ed bash Anything can be installed in that container, example, at first insert into a container and command:\napt-get update apt-get install fish apt-get install php #installs latest php Exit container directory\nexit Creates a image/snapshot of a [may be modified] container\ndocker commit \u0026lt;container_id\u0026gt; new_id_of_image Executing container commands Start a exited container\ndocker start \u0026lt;container id\u0026gt; Restart a crashed container\ndocker restart \u0026lt;contaienr_id\u0026gt; Start fish in interactive mode\ndocker exec -it \u0026lt;container_id\u0026gt; fish Start php inside container\ndocker exec -it \u0026lt;container_id\u0026gt; php -a echo \u0026#34;hello world\u0026#34;; exit Names of conatiners or Images Rename existing container\ndocker rename \u0026lt;old_name\u0026gt; new_name Giving the container a name, while rnning it\ndocker run -it --name newName \u0026lt;container_id\u0026gt; bash Copy files from a container Create a text file inside container. insert the container and command:\ntouch test.txt Copy from container. Exit from container and command:\ndocker cp containerName:/test.txt . #copies test.txt to host root Copy into container\ndocker cp ./test1.txt containerName:/ Container hostnames Gives a new hostname to the container. it doesn‚Äôt change the Names of container. but when user is inside the container, it sees the hostname, in this case, it‚Äôs root@test.\ndocker run -it -h test.local \u0026lt;image_id\u0026gt; bash Handling volumes It links between a directory of host and directory of container. in command, left side of ‚Äú:‚Äù defines the source of host direcotry and right side defines destination into container.In this case, changing in ~/data of directory of host machine also takes place changing in container automatically\nBind mounts, the pasth is absolute here\ndocker run -it --name test21 -v ~/data:/data \u0026lt;image_id\u0026gt; bash Volume help\ndocker volume --help Inspect a docker container\ndocker inspect \u0026lt;container_id\u0026gt; Named volume [non-bind mount], here the path is not absolute\ndocker run -it -v data:/data \u0026lt;container_id\u0026gt; bash Used volumes of local machine in container\ndocker volume ls If same volume is used in another container, they will share the volume.\ndocker run -it -v data:/data \u0026lt;container_id\u0026gt; bash //shares previous data volume Remove docker volume, volume_name is available in docker volume ls\ndocker volume rm \u0026lt;volume_name\u0026gt; Multiple volume\ndocker run -it -v data:/data -v myBin:/myBin \u0026lt;container_id\u0026gt; bash Copy volume from another container\ndocker run -it --name slave --volume-from \u0026lt;master_container_id/Name\u0026gt; \u0026lt;image_id\u0026gt; bash Inspect docker volume\ndocker volume inspect \u0026lt;volume_name\u0026gt; Show danglisg volumes [not used by any container]\ndocker volume ls -f dangling=true Remove one volume that is not used\ndocker volume rm \u0026lt;volume name\u0026gt; Removing all unsed volumes\ndocker volume prune Anonymous volume. ‚Äìrm will delete anonymous volume after remove container. here /foo is a anonymous volume\ndocker run --rm -v /foo -v awesome:/bar busybox top Untagged Images Show untagged images\ndocker images -f \u0026#34;dangling=true\u0026#34; -q Delete all untagged images\ndocker images -f \u0026#34;dangling=true\u0026#34; -q | xargs docker rmi Delete multiple images Delete images of same name\ndocker images --format \u0026#39;{{.Repository}}:{{.Tag}}\u0026#39; | grep \u0026#39;imagename\u0026#39; | xargs docker rmi # or, docker images -q imagename | uniq | xargs docker rmi --force Docker killer!!! :P Stop all container\ndocker ps -a -q | xargs docker stop Delete all container\ndocker ps -a -q | xargs docker rm Removing all unused volumes\ndocker volume prune Remove untagged dockers\ndocker images -q -f \u0026#34;dangling=true\u0026#34; | xargs docker rmi Remove stopped containers\ndocker ps -aq --no-trunc -f status=exited | xargs docker ‚Ä¶","date":1551391530,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551391530,"objectID":"c70cad698aad80d5b89321560e23c4d6","permalink":"https://maruftuhin.com/blog/my-docker-cheat-sheet/","publishdate":"2019-03-01T04:05:30+06:00","relpermalink":"/blog/my-docker-cheat-sheet/","section":"post","summary":"Essential `docker` commands.","tags":["Docker","Kubernetes","Bash"],"title":"My Docker Cheat Sheet","type":"post"},{"authors":[],"categories":["Kubernetes","MongoDB","minikube"],"content":"Replica-Sets Run MongoDB Replica Set on Kubernetes using Statefulset and PersistentVolumeClaim. Minikube kubernetes cluster is used for this post.\nCreate Secret for Key file MongoDB will use this key to communicate internal cluster.\n$ openssl rand -base64 741 \u0026gt; ./replica-sets/key.txt $ kubectl create secret generic shared-bootstrap-data --from-file=internal-auth-mongodb-keyfile=./replica-sets/key.txt secret \u0026#34;shared-bootstrap-data\u0026#34; created Deploy MongoDB Replica-Sets YAML apiVersion: v1 kind: Service metadata: name: mongodb-service labels: name: mongo spec: ports: - port: 27017 targetPort: 27017 clusterIP: None selector: role: mongo --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mongod spec: serviceName: mongodb-service replicas: 3 selector: matchLabels: role: mongo environment: test replicaset: MainRepSet template: metadata: labels: role: mongo environment: test replicaset: MainRepSet spec: containers: - name: mongod-container image: mongo:3.4 command: - \u0026#34;numactl\u0026#34; - \u0026#34;--interleave=all\u0026#34; - \u0026#34;mongod\u0026#34; - \u0026#34;--bind_ip\u0026#34; - \u0026#34;0.0.0.0\u0026#34; - \u0026#34;--replSet\u0026#34; - \u0026#34;MainRepSet\u0026#34; - \u0026#34;--auth\u0026#34; - \u0026#34;--clusterAuthMode\u0026#34; - \u0026#34;keyFile\u0026#34; - \u0026#34;--keyFile\u0026#34; - \u0026#34;/etc/secrets-volume/internal-auth-mongodb-keyfile\u0026#34; - \u0026#34;--setParameter\u0026#34; - \u0026#34;authenticationMechanisms=SCRAM-SHA-1\u0026#34; resources: requests: cpu: 0.2 memory: 200Mi ports: - containerPort: 27017 volumeMounts: - name: secrets-volume readOnly: true mountPath: /etc/secrets-volume - name: mongodb-persistent-storage-claim mountPath: /data/db volumes: - name: secrets-volume secret: secretName: shared-bootstrap-data defaultMode: 256 volumeClaimTemplates: - metadata: name: mongodb-persistent-storage-claim annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;standard\u0026#34; spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 1Gi Now Deploy the Yaml\n$ kc create -f ./replica-sets/mongodb-rc.yaml service \u0026#34;mongodb-service\u0026#34; created statefulset \u0026#34;mongod\u0026#34; created Wait for Pod running and PVC $ kubectl get all NAME DESIRED CURRENT AGE statefulsets/mongod 3 3 2m NAME READY STATUS RESTARTS AGE po/mongod-0 1/1 Running 0 2m po/mongod-1 1/1 Running 0 2m po/mongod-2 1/1 Running 0 2m $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongodb-persistent-storage-claim-mongod-0 Bound pvc-ba24dc66-319a-11e8-8dd9-080027779e8d 1Gi RWO standard 1h mongodb-persistent-storage-claim-mongod-1 Bound pvc-bf2e51a5-319a-11e8-8dd9-080027779e8d 1Gi RWO standard 1h mongodb-persistent-storage-claim-mongod-2 Bound pvc-c7948f87-319a-11e8-8dd9-080027779e8d 1Gi RWO standard 1h Setup ReplicaSet Configuration Finally, we need to connect to one of the ‚Äúmongod‚Äù container processes to configure the replica set.\nRun the following command to connect to the first container. In the shell initiate the replica set (we can rely on the hostnames always being the same, due to having employed a StatefulSet):\n$ kubectl exec -it mongod-0 -c mongod-container bash $ mongo \u0026gt; rs.initiate({_id: \u0026#34;MainRepSet\u0026#34;, version: 1, members: [ { _id: 0, host : \u0026#34;mongod-0.mongodb-service.default.svc.cluster.local:27017\u0026#34; }, { _id: 1, host : \u0026#34;mongod-1.mongodb-service.default.svc.cluster.local:27017\u0026#34; }, { _id: 2, host : \u0026#34;mongod-2.mongodb-service.default.svc.cluster.local:27017\u0026#34; } ]}); Keep checking the status of the replica set, with the following command, until the replica set is fully initialised and a primary and two secondaries are present:\n\u0026gt; rs.status(); # output similar to: { \u0026#34;set\u0026#34; : \u0026#34;MainRepSet\u0026#34;, \u0026#34;date\u0026#34; : ISODate(\u0026#34;2018-03-27T12:11:31.577Z\u0026#34;), \u0026#34;myState\u0026#34; : 2, \u0026#34;term\u0026#34; : NumberLong(1), \u0026#34;syncingTo\u0026#34; : \u0026#34;mongod-2.mongodb-service.default.svc.cluster.local:27017\u0026#34;, \u0026#34;heartbeatIntervalMillis\u0026#34; : NumberLong(2000), \u0026#34;optimes\u0026#34; : { \u0026#34;lastCommittedOpTime\u0026#34; : { \u0026#34;ts\u0026#34; : Timestamp(1522152676, 1), \u0026#34;t\u0026#34; : NumberLong(1) }, \u0026#34;appliedOpTime\u0026#34; : { \u0026#34;ts\u0026#34; : Timestamp(1522152686, 1), \u0026#34;t\u0026#34; : NumberLong(1) }, \u0026#34;durableOpTime\u0026#34; : { \u0026#34;ts\u0026#34; : Timestamp(1522152686, 1), \u0026#34;t\u0026#34; : NumberLong(1) } }, \u0026#34;members\u0026#34; : [ { \u0026#34;_id\u0026#34; : 0, \u0026#34;name\u0026#34; : \u0026#34;mongod-0.mongodb-service.default.svc.cluster.local:27017\u0026#34;, \u0026#34;health\u0026#34; : 1, \u0026#34;state\u0026#34; : 1, \u0026#34;stateStr\u0026#34; : \u0026#34;PRIMARY\u0026#34;, \u0026#34;uptime\u0026#34; : 399, \u0026#34;optime\u0026#34; : { \u0026#34;ts\u0026#34; : Timestamp(1522152686, 1), \u0026#34;t\u0026#34; : NumberLong(1) }, \u0026#34;optimeDurable\u0026#34; : { \u0026#34;ts\u0026#34; : Timestamp(1522152686, 1), \u0026#34;t\u0026#34; : NumberLong(1) }, \u0026#34;optimeDate\u0026#34; : ISODate(\u0026#34;2018-03-27T12:11:26Z\u0026#34;), \u0026#34;optimeDurableDate\u0026#34; : ISODate(\u0026#34;2018-03-27T12:11:26Z\u0026#34;), \u0026#34;lastHeartbeat\u0026#34; : ISODate(\u0026#34;2018-03-27T12:11:30.360Z\u0026#34;), \u0026#34;lastHeartbeatRecv\u0026#34; : ISODate(\u0026#34;2018-03-27T12:11:30.697Z\u0026#34;), \u0026#34;pingMs\u0026#34; : NumberLong(0), \u0026#34;electionTime\u0026#34; : Timestamp(1522152306, 1), \u0026#34;electionDate\u0026#34; : ISODate(\u0026#34;2018-03-27T12:05:06Z\u0026#34;), \u0026#34;configVersion\u0026#34; : 1 }, { \u0026#34;_id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;mongod-1.mongodb-service.default.svc.cluster.local:27017\u0026#34;, \u0026#34;health\u0026#34; : 1, \u0026#34;state\u0026#34; : 2, \u0026#34;stateStr\u0026#34; : \u0026#34;SECONDARY\u0026#34;, \u0026#34;uptime\u0026#34; : 505, \u0026#34;optime\u0026#34; : { \u0026#34;ts\u0026#34; : Timestamp(1522152686, 1), \u0026#34;t\u0026#34; : NumberLong(1) }, \u0026#34;optimeDate\u0026#34; : ISODate(\u0026#34;2018-03-27T12:11:26Z\u0026#34;), \u0026#34;syncingTo\u0026#34; : \u0026#34;mongod-2.mongodb-service.default.svc.cluster.local:27017\u0026#34;, \u0026#34;configVersion\u0026#34; : 1, \u0026#34;self\u0026#34; : true }, { \u0026#34;_id\u0026#34; : 2, \u0026#34;name\u0026#34; : ‚Ä¶","date":1547240870,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547240870,"objectID":"0acc8f44a4a00babc8f19ee0bb12bfa4","permalink":"https://maruftuhin.com/blog/mongodb-replica-set-on-kubernetes/","publishdate":"2019-01-12T03:07:50+06:00","relpermalink":"/blog/mongodb-replica-set-on-kubernetes/","section":"post","summary":"Run MongoDB Replica Set on Kubernetes using *Statefulset* and *PersistentVolumeClaim*. Minikube kubernetes cluster is used for this post.","tags":["Kubernetes","MongoDB","Database","minikube","kubectl"],"title":"Mongodb Replica Set on Kubernetes","type":"post"},{"authors":null,"categories":null,"content":"A tool to automate database deploy on Kubernetes using CRD. It also takes scheduled backup of databases.\nActed as KubeDB project head for more than a year. Conducted Bi-Weekly KubeDB Community meetings regularly. Added MongoDB, MySQL, Redis and Memcached support to KubeDB. Added MongoDB clustering system. Also, worked on PostgreSQL and Elasticsearch clustering. Worked with prometheus exporter to export database metrics. ","date":1518296775,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518296775,"objectID":"2891c0d7cc4ff94a3af34281cb5a677e","permalink":"https://maruftuhin.com/project/kubedb/","publishdate":"2018-02-11T03:06:15+06:00","relpermalink":"/project/kubedb/","section":"project","summary":"Run production-grade databases easily on **Kubernetes**","tags":["Professional","Kubernetes","OSS"],"title":"KubeDB","type":"project"},{"authors":null,"categories":null,"content":" Worked with docker-hub APIs to collect analytic data. Stored analytical datas in Google sheets using GO client. Used pixel tracking [aka, web beacon] to event each view of GitHub or other web pages in real time. ","date":1495832407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495832407,"objectID":"0cdb33103c04a01f57c434dbcf97727f","permalink":"https://maruftuhin.com/project/appscode_analytics/","publishdate":"2017-05-27T03:00:07+06:00","relpermalink":"/project/appscode_analytics/","section":"project","summary":"Different esential analytics in Appscode oss tools.","tags":["Professional","Kubernetes","OSS"],"title":"Appscode/Analytics","type":"project"},{"authors":null,"categories":null,"content":" Final year thesis \u0026amp; project. Developed for both PC and Android. Tools: Unity 5 Game Engine, Android SDK, C#. ","date":1485464787,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485464787,"objectID":"7b74efd48ac83abd8ef4eaf6dc441414","permalink":"https://maruftuhin.com/project/3d_racing_game/","publishdate":"2017-01-27T03:06:27+06:00","relpermalink":"/project/3d_racing_game/","section":"project","summary":"Final year thesis \u0026 project. Developed for both **PC** and **Android**.","tags":["Academic"],"title":"3D Racing Game","type":"project"},{"authors":null,"categories":null,"content":"","date":1466975403,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466975403,"objectID":"5322f09d41fdfe6bda3c0d41a8e12c2f","permalink":"https://maruftuhin.com/project/dsw/","publishdate":"2016-06-27T03:10:03+06:00","relpermalink":"/project/dsw/","section":"project","summary":"Academic project. Implemented using **C#** and **MySQL**.","tags":["Academic"],"title":"Department of Student Welfare (DSW) Management System","type":"project"},{"authors":null,"categories":null,"content":" Automatic Email verification project. Internship project. Tools: Java ","date":1459026651,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459026651,"objectID":"91ed5f9f13e9de08b95b0f76b2a7a922","permalink":"https://maruftuhin.com/project/email_verification/","publishdate":"2016-03-27T03:10:51+06:00","relpermalink":"/project/email_verification/","section":"project","summary":"Internship Project. Implemented using **Java**.","tags":["Professional"],"title":"Email Verification","type":"project"},{"authors":null,"categories":null,"content":"","date":1456521063,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456521063,"objectID":"80f057e50dd458d2e5bdde705b22216e","permalink":"https://maruftuhin.com/project/data_encryption/","publishdate":"2016-02-27T03:11:03+06:00","relpermalink":"/project/data_encryption/","section":"project","summary":"Data encryption project in **Java**.","tags":["Hobby"],"title":"Data Encryption Project","type":"project"},{"authors":null,"categories":null,"content":" Academical Database Project Tools: C# and Microsoft SQL Server. ","date":1424985014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1424985014,"objectID":"bfd6987a36052da23d5f37ab06c16fdb","permalink":"https://maruftuhin.com/project/library_system/","publishdate":"2015-02-27T03:10:14+06:00","relpermalink":"/project/library_system/","section":"project","summary":"Academic project. Implemented using **C#** and **Microsoft SQL Server**.","tags":["Academic"],"title":"Library Management System","type":"project"},{"authors":null,"categories":null,"content":"","date":1330290627,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1330290627,"objectID":"1cfe293321ef3cd90cdf44b7e435c1a3","permalink":"https://maruftuhin.com/project/scientific_calculator/","publishdate":"2012-02-27T03:10:27+06:00","relpermalink":"/project/scientific_calculator/","section":"project","summary":"Academic project. Implemented using **Java swing**.","tags":["Academic"],"title":"Scientific Calculator","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://maruftuhin.com/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"","tags":null,"title":"","type":"page"}]